{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from tqdm.notebook import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient, models, conversions\n",
    "from langchain.docstore.document import Document as LangchainDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(path=\"medicalqna.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5213/3471414559.py:1: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.recreate_collection(\n",
    "    collection_name=\"medicalqna\",\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=encoder.get_sentence_embedding_dimension(),  # Vector size is defined by used model\n",
    "        distance=models.Distance.COSINE,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"ruslanmv/ai-medical-chatbot\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Description', 'Patient', 'Doctor'],\n",
       "    num_rows: 256916\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd18dad46bd46f1ad19620afe3ebd35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"Description\"], metadata={\"question\": doc[\"Patient\"], \"answer\": doc[\"Doctor\"]})\n",
    "    for doc in tqdm(ds)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178bc494b01f488c9bac9a311a148143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/assignment/db.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://hameessayed-assignment-cvh4lidmis1.ws-us115.gitpod.io/workspace/assignment/db.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, doc \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(RAW_KNOWLEDGE_BASE), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(RAW_KNOWLEDGE_BASE)):\n\u001b[1;32m      <a href='vscode-notebook-cell://hameessayed-assignment-cvh4lidmis1.ws-us115.gitpod.io/workspace/assignment/db.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     content \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'''\u001b[39m\u001b[39m{\u001b[39;00mdoc\u001b[39m.\u001b[39mpage_content\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mdoc\u001b[39m.\u001b[39mmetadata[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mdoc\u001b[39m.\u001b[39mmetadata[\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'''\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://hameessayed-assignment-cvh4lidmis1.ws-us115.gitpod.io/workspace/assignment/db.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     vector \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mencode(content)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m      <a href='vscode-notebook-cell://hameessayed-assignment-cvh4lidmis1.ws-us115.gitpod.io/workspace/assignment/db.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     payload \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m: doc\u001b[39m.\u001b[39mmetadata[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m: doc\u001b[39m.\u001b[39mmetadata[\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpage_content\u001b[39m\u001b[39m\"\u001b[39m: doc\u001b[39m.\u001b[39mpage_content}\n\u001b[1;32m      <a href='vscode-notebook-cell://hameessayed-assignment-cvh4lidmis1.ws-us115.gitpod.io/workspace/assignment/db.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     point \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mPointStruct(\u001b[39mid\u001b[39m\u001b[39m=\u001b[39midx, vector\u001b[39m=\u001b[39mvector, payload\u001b[39m=\u001b[39mpayload)\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:517\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    514\u001b[0m features\u001b[39m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    516\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 517\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[1;32m    518\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    519\u001b[0m         out_features \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    220\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:118\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[1;32m    116\u001b[0m     trans_features[\u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    119\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m    121\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m\"\u001b[39m: output_tokens, \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m: features[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]})\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1141\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1142\u001b[0m     embedding_output,\n\u001b[1;32m   1143\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1144\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1145\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1146\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1147\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1148\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1149\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1150\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1151\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1152\u001b[0m )\n\u001b[1;32m   1153\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1154\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    684\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    685\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m         output_attentions,\n\u001b[1;32m    692\u001b[0m     )\n\u001b[1;32m    693\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    695\u001b[0m         hidden_states,\n\u001b[1;32m    696\u001b[0m         attention_mask,\n\u001b[1;32m    697\u001b[0m         layer_head_mask,\n\u001b[1;32m    698\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    699\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    700\u001b[0m         past_key_value,\n\u001b[1;32m    701\u001b[0m         output_attentions,\n\u001b[1;32m    702\u001b[0m     )\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    705\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:584\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    573\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    574\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    582\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    585\u001b[0m         hidden_states,\n\u001b[1;32m    586\u001b[0m         attention_mask,\n\u001b[1;32m    587\u001b[0m         head_mask,\n\u001b[1;32m    588\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    589\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    590\u001b[0m     )\n\u001b[1;32m    591\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:514\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    505\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    506\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 514\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    515\u001b[0m         hidden_states,\n\u001b[1;32m    516\u001b[0m         attention_mask,\n\u001b[1;32m    517\u001b[0m         head_mask,\n\u001b[1;32m    518\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    519\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    520\u001b[0m         past_key_value,\n\u001b[1;32m    521\u001b[0m         output_attentions,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    524\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:408\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey(current_states))\n\u001b[0;32m--> 408\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue(current_states))\n\u001b[1;32m    409\u001b[0m     \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_cross_attention:\n\u001b[1;32m    410\u001b[0m         key_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m0\u001b[39m], key_layer], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:250\u001b[0m, in \u001b[0;36mBertSelfAttention.transpose_for_scores\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranspose_for_scores\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 250\u001b[0m     new_x_shape \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_head_size)\n\u001b[1;32m    251\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(new_x_shape)\n\u001b[1;32m    252\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "points = []\n",
    "for idx, doc in tqdm(enumerate(RAW_KNOWLEDGE_BASE), total=len(RAW_KNOWLEDGE_BASE)):\n",
    "    content = f'''{doc.page_content}: {doc.metadata[\"question\"]}: {doc.metadata[\"answer\"]}'''\n",
    "    vector = encoder.encode(content).tolist()\n",
    "    payload = {\"question\": doc.metadata[\"question\"], \"answer\": doc.metadata[\"answer\"], \"page_content\": doc.page_content}\n",
    "    point = models.PointStruct(id=idx, vector=vector, payload=payload)\n",
    "    points.append(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointStruct(id=0, vector=[-0.004023421090096235, -0.005357899237424135, 0.06804246455430984, 0.066102996468544, -0.07491514086723328, -0.0009945608908310533, -0.011394156143069267, 0.08904100209474564, -0.024408191442489624, 0.04369188845157623, 0.02114597149193287, 0.10576315969228745, -0.021432047709822655, 0.0069670709781348705, -0.05428933724761009, 0.06488518416881561, -0.040500059723854065, -0.04325779527425766, -0.018332326784729958, 0.0813630223274231, 0.0723172053694725, 0.01802097074687481, -0.013998948968946934, -0.012376553378999233, -0.07446275651454926, 0.015276233665645123, -0.008094154298305511, -0.029375070706009865, 0.09893211722373962, -0.01204764936119318, 0.021430907770991325, 0.05923422425985336, -0.01596364565193653, -0.0030213678255677223, -0.0162824559956789, 0.0483940914273262, -0.16837115585803986, -0.02928023971617222, -0.05569662153720856, -0.017170332372188568, 0.056520309299230576, -0.03506201505661011, 0.013154755346477032, 0.018889956176280975, 0.04319768771529198, -0.0055688428692519665, 0.027937250211834908, -0.026492660865187645, -0.040684860199689865, 0.011079451069235802, 0.025058871135115623, -0.0338127426803112, 0.0314568430185318, 0.047425247728824615, -0.0412730909883976, -0.018882960081100464, -0.05784498527646065, 0.042579807341098785, -0.0557829923927784, 0.09383177757263184, 0.10714518278837204, -0.003938788082450628, 0.021437449380755424, 0.002405554987490177, -0.022111035883426666, -0.05313311517238617, 0.07963768392801285, -0.11736241728067398, 0.016479656100273132, -0.03971200808882713, 0.041723087430000305, -0.12152300775051117, 0.009504406712949276, -0.049014218151569366, 0.015242079272866249, -0.06895666569471359, 0.012433811090886593, 0.011030222289264202, 0.007727138698101044, -0.06615910679101944, 0.010548935271799564, 0.0278947614133358, -0.020486069843173027, 0.0061761667020618916, -0.027440844103693962, -0.03624136745929718, 0.045835960656404495, -0.04589245840907097, -0.01890665851533413, 0.07901323586702347, 0.11621719598770142, -0.005328687839210033, -0.05443514883518219, 0.001576108974404633, 0.017614835873246193, -0.012367491610348225, -0.06068931147456169, -0.009926979430019855, -0.08647532761096954, 0.002416296862065792, -0.004709297325462103, 0.010334055870771408, -0.03297705575823784, 0.07206922769546509, -0.019556676968932152, 0.005505789536982775, -0.042124077677726746, -0.09783060103654861, -0.05214643478393555, -0.01639959029853344, -0.010570349171757698, 0.05043083801865578, -0.029172083362936974, -0.0032360891345888376, -0.029824350029230118, 0.015334834344685078, 0.010674385353922844, 0.0354912094771862, 0.05608445033431053, 0.007102256175130606, -0.0018570380052551627, -0.0302572138607502, -0.05686226859688759, -0.033646438270807266, -0.018382159993052483, -0.07466639578342438, -0.08220823109149933, -3.5585876398977424e-33, 0.0379302017390728, 0.008918965235352516, -0.1132366806268692, -0.09933875501155853, -0.12995141744613647, -0.005512397736310959, -0.011072397232055664, -0.05723712593317032, 0.04036084935069084, -0.01002364419400692, 0.02682451717555523, 0.028493177145719528, 0.053998541086912155, -0.06089167296886444, -0.031920671463012695, 0.026911187916994095, 0.0042554838582873344, -0.017498739063739777, 0.007283184677362442, -0.037507038563489914, 0.014172126539051533, 0.09241390973329544, -0.014864383265376091, 0.022601069882512093, -0.07409737259149551, -0.07652483880519867, -0.012737222015857697, -0.07688350230455399, 0.029553880915045738, -0.008858905173838139, 0.045514415949583054, -0.0015810792101547122, 0.054429005831480026, -0.06056029349565506, -0.0012454460375010967, 0.017597302794456482, 0.11056479811668396, 0.009112950414419174, -0.03866290673613548, -0.01677796058356762, 0.0033386261202394962, 0.050232164561748505, 0.013067795895040035, 0.024297375231981277, 0.041770149022340775, 0.01581709086894989, -0.02558581903576851, -0.025949208065867424, -0.008954059332609177, -0.028393998742103577, 0.0005183666362427175, -0.005991228856146336, 0.04119288548827171, 0.0021528461948037148, -0.012315021827816963, 0.023558543995022774, -0.027908118441700935, -0.015950465574860573, 0.047800567001104355, 0.04254473000764847, 0.025778753682971, -0.1447533220052719, 0.01966516301035881, 0.017438793554902077, -0.01916174404323101, -0.04689610004425049, 0.010633089579641819, 0.04149125516414642, -0.033833809196949005, -0.030301684513688087, -0.11065123975276947, -0.007684117648750544, -0.009461283683776855, 0.08759089559316635, -0.12176616489887238, 0.0308950487524271, 0.03093479387462139, 0.09775692969560623, 0.013814546167850494, -0.038482099771499634, -0.15667696297168732, -0.018248120322823524, 0.010907124727964401, 0.07544831186532974, 0.06207239255309105, 0.052294887602329254, -0.06495638936758041, -0.06535962969064713, 0.054093096405267715, -0.0166015587747097, 0.035284169018268585, 0.019046371802687645, -0.019631486386060715, 0.026772215962409973, 0.05114732310175896, -7.221245468093628e-34, -0.006932348012924194, 0.027475643903017044, -0.056443583220243454, -0.0424654521048069, 0.0483730249106884, -0.00018619131878949702, -0.10376131534576416, 0.07930368185043335, -0.07266862690448761, -0.06554077565670013, -0.08658415824174881, 0.014733351767063141, -0.021656615659594536, -0.02760012075304985, -0.08418985456228256, -0.02028137817978859, -0.06275121122598648, -0.005112855229526758, -0.13781562447547913, 0.07322820276021957, -0.040646012872457504, 0.14426709711551666, 0.048897381871938705, -0.02232244610786438, 0.06453841179609299, 0.030304815620183945, 0.031965043395757675, -0.010784703306853771, -0.019697578623890877, 0.03599528223276138, -0.0030392808839678764, -0.09401217848062515, -0.016302363947033882, 0.016477303579449654, -0.026759536936879158, 0.062473755329847336, 0.06761112064123154, -0.07934490591287613, -0.043310098350048065, -0.04673183336853981, 0.057428985834121704, 0.028492094948887825, 0.02207810990512371, -0.01525973156094551, 0.025850456207990646, -0.06251003593206406, -0.11268635839223862, 0.028234435245394707, 0.006053320597857237, -0.0180906243622303, 0.055380597710609436, 0.04887298867106438, 0.10910118371248245, 0.007882427424192429, 0.03875076770782471, -0.052039071917533875, 0.07169496268033981, -0.04508837312459946, -0.01417431328445673, 0.005249528679996729, -0.02175329253077507, 0.012076761573553085, 0.03227904438972473, -0.04345656931400299, 0.012486572377383709, 0.03842118754982948, -0.01903076469898224, -0.06635642051696777, -0.04510784149169922, 0.006093378644436598, -0.10056982934474945, 0.02999844029545784, 0.016919312998652458, 0.0007200289983302355, 0.04383222386240959, 0.07550336420536041, 0.01006706990301609, -0.1142793595790863, 0.02701140008866787, -0.009933441877365112, 0.029697807505726814, 0.020815597847104073, 0.051442161202430725, 0.03816734999418259, -0.05354457348585129, -0.009777027182281017, 0.03686535730957985, 0.055483706295490265, 0.002861256478354335, 0.00018817782984115183, 0.010922775603830814, 0.003821494523435831, -0.09795646369457245, 0.00634716497734189, 0.018493307754397392, -4.392713037759677e-08, -0.08980751037597656, -0.02006376162171364, -0.010354293510317802, -0.035705260932445526, 0.03629837930202484, -0.042886584997177124, -0.00303675327450037, 0.041895169764757156, -0.04244157299399376, -0.03178403899073601, -0.0006460996810346842, 0.06588629633188248, -0.021570960059762, -0.023278728127479553, 0.008013426326215267, 0.007788906339555979, -0.019154468551278114, 0.08425574004650116, -0.012964092195034027, -0.04184603691101074, -0.08141687512397766, -0.06511805951595306, 0.09141331911087036, 0.035617031157016754, 0.010364101268351078, 0.05148971453309059, -0.03154135122895241, 0.016172826290130615, -0.06108944118022919, -0.01964609883725643, 0.11871311813592911, 0.04776301607489586, 0.09600009769201279, -0.0014963323483243585, -0.03987637534737587, 0.06595561653375626, 0.09123311191797256, -0.05480990558862686, -0.06450687348842621, 0.042093973606824875, -0.04071207344532013, -0.0004117133794352412, 0.09329728037118912, 0.020796064287424088, -0.05502871051430702, -0.07444942742586136, 0.0320802666246891, 0.043847326189279556, 0.04353643208742142, -0.023249857127666473, 0.08163034915924072, -0.02232000045478344, -0.04717634990811348, -0.0057684434577822685, -0.10387445241212845, 0.06626199930906296, 0.04095855727791786, 0.07005620002746582, -0.012230693362653255, 0.005471067037433386, 0.02827947586774826, 0.0005595762049779296, 0.08990170806646347, -0.015852821990847588], payload={'question': 'Hi doctor,I am just wondering what is abutting and abutment of the nerve root means in a back issue. Please explain. What treatment is required for\\xa0annular bulging and tear?', 'answer': 'Hi. I have gone through your query with diligence and would like you to know that I am here to help you. For further information consult a neurologist online -->', 'page_content': 'Q. What does abutment of the nerve root mean?'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3870611/1009793423.py:1: DeprecationWarning: `upload_records` is deprecated, use `upload_points` instead\n",
      "  client.upload_records(\n"
     ]
    }
   ],
   "source": [
    "client.upload_records(\n",
    "    collection_name=\"medicalqna\",\n",
    "    records=points,\n",
    "    parallel=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = client.search(\n",
    "    collection_name=\"medicalqna\",\n",
    "    query_vector=encoder.encode(\"how to perform cpr?\").tolist(),\n",
    "    limit=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\n",
    "for hit in hits:\n",
    "    context += hit.payload['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Cardiac arrest requires immediate CPR if necessary ACLS or ALS(Advanced Life Support).  However in case of Bronchial Asthma if a patients develop Respiratory fatigue and his Oxygen concentration in blood starts falling put him on a ventilator, but wean him off as early as possible OR if he develops Cyanosis CPR + ALS.  There are other protocols for all the above disease but you stick to cardiac arrest and asthma.  Don't forget Cardiac Asthma, severe cases may requires ACLS.  So it is the indication and symtomology of the patient that gives rise to CPR, ACLS etc. no hard and fast rule.Hi. First of all, I appreciate your interest to know the emergency life-saving procedure. Tongue bite usually occurs in seizures and not in heart attacks. There is a possibility of a seizure in her case. Many reasons can be there for such an incident. It can be due to fall in sugar levels, due to side effect of asthalin in medicines, due to missed drug, due to high potassium caused by respiratory drugs, due to high blood pressure causing brain stroke, etc. If there is a heart attack or cardiac arrest, then the best way is to give CPR (Cardiopulmonary Resuscitation). Do not rub, but compress the chest with your hand and release as if you are manually pumping blood. Also, you should have given mouth to mouth respiration if you thought she is intermittently breathing. This way you are pushing air into lungs manually. The purpose of CPR is to allow lungs and heart keep working till we can get the patient onto a machine. Since the diagnosis here is not clear, we would suggest that what you did was in the right direction but you should have applied more pressure and done it aggressively. Please learn basic CPR from any institute or doctor so that next time you know how to save someone!!Hello, As per your history, it can be anything from first heart attack to cardiac arrest to any life-threatening arrhythmia. As she is in hospital, let's hope for the best. Hope I have answered your query. Let me know if I can assist you further. Take care Regards, Dr Varinder Joshi , General & Family Physicianhello, thanks for your query, yes. timely CPR will save in sudden cardiac arrest due to trauma( road accident). c to that the pattern of CPR appropriate. all the best. take care.Child with CP if not treated well since earliest possible can cause problems like you are facing with your child. Difficulty in walking can be improved by using advanced method of physiotherapy and training machines. Further treatment for this include botox injections and surgeries to release contracture if developed. A lot improvement can be expected if child is shown early and to a expertee.  However get your child evaluated in ncr please go to IHBAS Hospital Shahdra or Lok nayak hospital ito\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardiac arrest requires immediate CPR if necessary ACLS or ALS(Advanced Life Support).  However in case of Bronchial Asthma if a patients develop Respiratory fatigue and his Oxygen concentration in blood starts falling put him on a ventilator, but wean him off as early as possible OR if he develops Cyanosis CPR + ALS.  There are other protocols for all the above disease but you stick to cardiac arrest and asthma.  Don't forget Cardiac Asthma, severe cases may requires ACLS.  So it is the indication and symtomology of the patient that gives rise to CPR, ACLS etc. no hard and fast rule.\n",
      "Hi. First of all, I appreciate your interest to know the emergency life-saving procedure. Tongue bite usually occurs in seizures and not in heart attacks. There is a possibility of a seizure in her case. Many reasons can be there for such an incident. It can be due to fall in sugar levels, due to side effect of asthalin in medicines, due to missed drug, due to high potassium caused by respiratory drugs, due to high blood pressure causing brain stroke, etc. If there is a heart attack or cardiac arrest, then the best way is to give CPR (Cardiopulmonary Resuscitation). Do not rub, but compress the chest with your hand and release as if you are manually pumping blood. Also, you should have given mouth to mouth respiration if you thought she is intermittently breathing. This way you are pushing air into lungs manually. The purpose of CPR is to allow lungs and heart keep working till we can get the patient onto a machine. Since the diagnosis here is not clear, we would suggest that what you did was in the right direction but you should have applied more pressure and done it aggressively. Please learn basic CPR from any institute or doctor so that next time you know how to save someone!!\n",
      "Hello, As per your history, it can be anything from first heart attack to cardiac arrest to any life-threatening arrhythmia. As she is in hospital, let's hope for the best. Hope I have answered your query. Let me know if I can assist you further. Take care Regards, Dr Varinder Joshi , General & Family Physician\n",
      "hello, thanks for your query, yes. timely CPR will save in sudden cardiac arrest due to trauma( road accident). c to that the pattern of CPR appropriate. all the best. take care.\n",
      "Child with CP if not treated well since earliest possible can cause problems like you are facing with your child. Difficulty in walking can be improved by using advanced method of physiotherapy and training machines. Further treatment for this include botox injections and surgeries to release contracture if developed. A lot improvement can be expected if child is shown early and to a expertee.  However get your child evaluated in ncr please go to IHBAS Hospital Shahdra or Lok nayak hospital ito\n"
     ]
    }
   ],
   "source": [
    "for hit in hits:\n",
    "    print(hit.payload['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
